{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dropout_MNIST_test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYBSbdO2ry-R",
        "colab_type": "code",
        "outputId": "b5357a80-4ef1-481c-db0f-366c24f5a5cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/f9/0626bbdb322e3a078d968e87e3b01341e7890544de891d0cb613641220e6/ipython-autotime-0.1.tar.bz2\n",
            "Building wheels for collected packages: ipython-autotime\n",
            "  Building wheel for ipython-autotime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipython-autotime: filename=ipython_autotime-0.1-cp36-none-any.whl size=1832 sha256=bedcc9d34c604448f2c7ff1388e37081e2ee5b3386144387af518b525daff12b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/df/81/2db1e54bc91002cec40334629bc39cfa86dff540b304ebcd6e\n",
            "Successfully built ipython-autotime\n",
            "Installing collected packages: ipython-autotime\n",
            "Successfully installed ipython-autotime-0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CcKVmc7r33c",
        "colab_type": "code",
        "outputId": "dd61aa75-a280-4240-da0a-371eca7b5c73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from IPython import display\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.02 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SpFh6V7r5CY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "hyperparams = {'l1_out': 512,\n",
        "                  'l2_out': 512,\n",
        "                  'l1_drop': 0.5,\n",
        "                  'l2_drop': 0.5,\n",
        "                  'batch_size': 32,\n",
        "                  'epochs': 5}\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('mnist-data/', train=True, download=True,\n",
        "                       transform=transforms.Compose([transforms.ToTensor(),])),\n",
        "        batch_size=hyperparams['batch_size'], shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('mnist-data/', train=False, transform=transforms.Compose([transforms.ToTensor(),])\n",
        "                       ),\n",
        "        batch_size=hyperparams['batch_size'], shuffle=True)\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, hyperparams['l1_out'])\n",
        "        self.dropout1 = nn.Dropout(hyperparams['l1_drop'])\n",
        "        self.fc2 = nn.Linear(hyperparams['l1_out'], hyperparams['l2_out'])\n",
        "        self.dropout2 = nn.Dropout(hyperparams['l2_drop'])\n",
        "        self.fc3 = nn.Linear(hyperparams['l2_out'], 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x) #exponentiate logits to retrieve predicted probabilities\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm6m1XZgsBkl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "8ad8a57f-e733-40f2-ae22-6d0728d60a3f"
      },
      "source": [
        "net_trained = Net()\n",
        "net_trained.train()\n",
        "# create an Adam optimizer\n",
        "optimizer = optim.Adam(net_trained.parameters(), lr=0.001)\n",
        "# create a loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = hyperparams['epochs']\n",
        "\n",
        "# run the main training loop\n",
        "for epoch in range(epochs):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n",
        "        data = data.view(-1, 28*28)\n",
        "        optimizer.zero_grad()\n",
        "        net_out = net_trained(data)\n",
        "        loss = criterion(net_out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 500 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                           100. * batch_idx / len(train_loader), loss.data))\n",
        "            "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.300746\n",
            "Train Epoch: 0 [16000/60000 (27%)]\tLoss: 0.083377\n",
            "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.186018\n",
            "Train Epoch: 0 [48000/60000 (80%)]\tLoss: 0.311338\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.013656\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.024796\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.011382\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.049497\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.021566\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.006836\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.038105\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.055847\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.001605\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.017032\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.001622\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.007173\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.002055\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.001304\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.207564\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.002137\n",
            "time: 2min 5s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJK6jguUsDYu",
        "colab_type": "code",
        "outputId": "e6879999-52c1-49ff-8ec9-255cc71b7e0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "f = 'model'\n",
        "torch.save(net_trained.state_dict(), f)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 6.13 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv4dDRMJsFQC",
        "colab_type": "code",
        "outputId": "9886d57a-b9f7-4eec-bc86-913c220223b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "# try this on just ONE new image from the test set\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "\n",
        "for test_images, test_labels in test_loader:  \n",
        "    sample_image = test_images[0]    # Reshape them according to your needs.\n",
        "    sample_label = test_labels[0]\n",
        "\n",
        "sample_image_modified = sample_image.permute(1, 2, 0)\n",
        "plt.imshow(sample_image_modified.numpy().squeeze())\n",
        "plt.show()\n",
        "print(sample_label.item())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7EAShB1IgrADSRB2IAnCDiTxf5+b97kIKS7gAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "time: 814 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFdf7PDbsH6e",
        "colab_type": "code",
        "outputId": "bfd475bd-70fc-4e5c-d7b0-293893c615db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "trained = False\n",
        "net_trained.load_state_dict(torch.load(f))\n",
        "net_untrained = Net()\n",
        "\n",
        "\n",
        "if trained:\n",
        "  output = net_trained(sample_image.flatten()) # sample image has dimensions 28*28, needs to be 784\n",
        "  #print(output) # list of 10 elements; class-specific logits pre-activation.\n",
        "  print(sample_label.item())\n",
        "  predictive_probs = np.exp(output.detach())\n",
        "  print(predictive_probs)\n",
        "\n",
        "  _, prediction = output.max(0)\n",
        "  print(prediction.item())\n",
        "else:\n",
        "  output = net_untrained(sample_image.flatten()) # sample image has dimensions 28*28, needs to be 784\n",
        "  #print(output) # list of 10 elements; class-specific logits pre-activation.\n",
        "  print(sample_label.item())\n",
        "  predictive_probs = np.exp(output.detach())\n",
        "  print(predictive_probs)\n",
        "\n",
        "  _, prediction = output.max(0)\n",
        "  print(prediction.item())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "tensor([0.0933, 0.1063, 0.0996, 0.0908, 0.1031, 0.0969, 0.1001, 0.0994, 0.1060,\n",
            "        0.1046])\n",
            "1\n",
            "time: 20.7 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NsALThC3dBr",
        "colab_type": "text"
      },
      "source": [
        "See\n",
        "https://gitlab.com/wdeback/dl-keras-tutorial/blob/master/notebooks/3-cnn-segment-retina-uncertainty.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjRb1EsjP_Vm",
        "colab_type": "code",
        "outputId": "28843d28-90af-404b-e4ae-9325fcaa9abd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "T = 3 # how many times to apply drop-out at test time\n",
        "\n",
        "def apply_dropout(m):\n",
        "    if type(m) == nn.Dropout:\n",
        "        m.train()\n",
        "\n",
        "def uncertainties(p):\n",
        "    aleatoric = np.mean(p*(1-p), axis=0)\n",
        "    epistemic = np.mean(p**2, axis=0) - np.mean(p, axis=0)**2\n",
        "    return aleatoric, epistemic\n",
        "\n",
        "\n",
        "def predict(model, image, label, T=T):\n",
        "    model.eval()\n",
        "    model.apply(apply_dropout) # STILL NOT WORKING WITH DROPOUT AT TEST-TIME\n",
        "    # prepare label\n",
        "    label = label.item()\n",
        "    # prepare image\n",
        "    image = image.flatten()\n",
        "    standard_output = model(image)\n",
        "    _, standard_prediction = standard_output.max(0)\n",
        "    # each vector will consist of T elements- the class-specific predictive probability from each model\n",
        "    zero_p_hat= []\n",
        "    one_p_hat= []\n",
        "    two_p_hat= []\n",
        "    three_p_hat= []\n",
        "    four_p_hat= []\n",
        "    five_p_hat= []\n",
        "    six_p_hat= []\n",
        "    seven_p_hat= []\n",
        "    eight_p_hat= []\n",
        "    nine_p_hat= []\n",
        "\n",
        "    # predict stochastic dropout model T times\n",
        "    for t in range(T):\n",
        "        output = model(image)\n",
        "        output = output.detach()\n",
        "        output_prob = np.exp(output) #convert to predictive probabilities\n",
        "\n",
        "        zero_p_hat.append(output_prob[0].item()) # P( c = 0 | image)\n",
        "        one_p_hat.append(output_prob[1].item()) # P( c = 1 | image)\n",
        "        two_p_hat.append(output_prob[2].item()) # P( c = 2 | image)\n",
        "        three_p_hat.append(output_prob[3].item()) # P( c = 3 | image)\n",
        "        four_p_hat.append(output_prob[4].item()) # P( c = 4 | image)\n",
        "        five_p_hat.append(output_prob[5].item()) # P( c = 5 | image)\n",
        "        six_p_hat.append(output_prob[6].item()) # P( c = 6 | image)\n",
        "        seven_p_hat.append(output_prob[7].item()) # P( c = 7 | image)\n",
        "        eight_p_hat.append(output_prob[8].item()) # P( c = 8 | image)\n",
        "        nine_p_hat.append(output_prob[9].item()) # P( c = 9 | image)\n",
        "\n",
        "    # mean prediction\n",
        "    zero_var = np.var(zero_p_hat)\n",
        "    one_var = np.var(one_p_hat)\n",
        "    two_var = np.var(two_p_hat)\n",
        "    three_var = np.var(three_p_hat)\n",
        "    four_var = np.var(four_p_hat)\n",
        "    five_var = np.var(five_p_hat)\n",
        "    six_var = np.var(six_p_hat)\n",
        "    seven_var = np.var(seven_p_hat)\n",
        "    eight_var = np.var(eight_p_hat)\n",
        "    nine_var = np.var(nine_p_hat)\n",
        "    \n",
        "    # will be a list of 10 elements, the variance of each class's predictions across T models\n",
        "    class_specific_uncertainties = [zero_var, one_var, two_var, three_var, four_var, five_var, six_var, seven_var, eight_var, nine_var]\n",
        "    variance_of_class_specific_predictions = class_specific_uncertainties[label]\n",
        "    \n",
        "    # estimate uncertainties (eq. 4 )\n",
        "    # eq.4 in https://openreview.net/pdf?id=Sk_P2Q9sG\n",
        "    # see https://github.com/ykwon0407/UQ_BNN/issues/1\n",
        "    p_hat_lists = [zero_p_hat, one_p_hat, two_p_hat, three_p_hat, four_p_hat, five_p_hat, six_p_hat, seven_p_hat, eight_p_hat, nine_p_hat]\n",
        "    epistemic, aleatoric = uncertainties(np.array(p_hat_lists[label]))\n",
        "\n",
        "    return standard_prediction.item(), variance_of_class_specific_predictions, np.squeeze(aleatoric), np.squeeze(epistemic), p_hat_lists[label]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 53.8 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWg1FnvdSMan",
        "colab_type": "code",
        "outputId": "c5bcb705-22fc-4369-c65a-b7581905c7a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "prediction, var_uncertainty, aleatoric, epistemic, _ = predict(net_untrained, sample_image, sample_label)\n",
        "print(\"\\n The model predicts: {} \\n The ground truth is {}.\\n With drop-out at test-time {} times, variance of class-specific predictions across the models is {}. \\n Finally, aleatoric and epistemic uncertainties are {} and {}.\\n The models give predictive probabilities: {}\".format(prediction, sample_label, T, var_uncertainty, aleatoric, epistemic, _))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " The model predicts: 1 \n",
            " The ground truth is 6.\n",
            " With drop-out at test-time 3 times, variance of class-specific predictions across the models is 0.0. \n",
            " Finally, aleatoric and epistemic uncertainties are 0.0 and 0.09009235121962117.\n",
            " The models give predictive probabilities: [0.10011545568704605, 0.10011545568704605, 0.10011545568704605]\n",
            "time: 9.18 ms\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}